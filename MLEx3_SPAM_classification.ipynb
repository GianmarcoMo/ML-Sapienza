{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLEx3.SPAM_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-o2cEpeQZjs"
      },
      "source": [
        "# Machine Learning - Exercise 3\n",
        "# SMS SPAM classification\n",
        "\n",
        "*   download the dataset available at this [link](https://drive.google.com/a/diag.uniroma1.it/file/d/17YZemn1MidhFA0-wenfVolZAwclLRUXM/view)\n",
        "*   copy the dataset in a folder of your personal Drive\n",
        "*   mount your Google Drive (more details will follow)\n",
        "*   set the correct path for loading the dataset (more details will follow)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z62YJE78EcK9"
      },
      "source": [
        "## Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXZGno8QYOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8459eda-e040-4be8-8427-fb54da1b32fe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.naive_bayes import *\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print('Libraries imported.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnm5HakC6dO0"
      },
      "source": [
        "## Load data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otk6cVVo6mR8"
      },
      "source": [
        "To load the file set the correct path of the dataset located in your drive. Once mounted, your drive works like a Linux system, so you can check folders etc... running commands like `ls` or `cd` preceded by `%`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvPjvjXGQ0sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db38ced-9896-4b71-e360-9572ff419c80"
      },
      "source": [
        "# open the dataset in sample_data\n",
        "filename = '/content/sample_data/SMSSpamCollection'\n",
        "db = pd.read_csv(filename, sep='\\t', header=None, names=['label', 'text'])\n",
        "print('File in '+filename+' loaded: %d samples.' %(len(db.label)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File in /content/sample_data/SMSSpamCollection loaded: 5572 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns_oBwUo64tF"
      },
      "source": [
        "Show a random sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhl2FLCKSUmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f6700d-13a8-4d43-e201-36b1c56f339b"
      },
      "source": [
        "id = random.randrange(0,len(db.label))\n",
        "print('ID: %d\\nLabel: %s\\nDescription: %s' %(id, db.label[id], db.text[id]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 1507\n",
            "Label: spam\n",
            "Description: Thanks for the Vote. Now sing along with the stars with Karaoke on your mobile. For a FREE link just reply with SING now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uBD7RsJ7bzB"
      },
      "source": [
        "## Choose vectorizer\n",
        "\n",
        "Compute vectorizer terms for all messages. More info:\n",
        "\n",
        "\n",
        "\n",
        "*   [Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n",
        "*   [Count](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "*   [Tfid](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKWEucDKWnDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d5217a-2909-46e9-f869-742d9cc89112"
      },
      "source": [
        "vectorizer_type = \"count\" # \"hashing\", \"count\" or \"tfid\"\n",
        "\n",
        "if vectorizer_type == \"hashing\":\n",
        "  vectorizer = HashingVectorizer(stop_words='english') # multivariate\n",
        "elif vectorizer_type == \"count\":\n",
        "  vectorizer = CountVectorizer(stop_words='english') # multinomial\n",
        "elif vectorizer_type == \"tfid\":\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X_all = vectorizer.fit_transform(db.text)\n",
        "y_all = db.label\n",
        "\n",
        "print(X_all.shape)\n",
        "print(y_all.shape)\n",
        "\n",
        "# HOMEWORK - EVALUATE ALL OF METHODS\n",
        "# Calculate each methods\n",
        "print('\\n--- Homework')\n",
        "print('Hashing Vectorizer')\n",
        "vectorizer_1 = HashingVectorizer(stop_words='english')\n",
        "X_all_1 = vectorizer_1.fit_transform(db.text)\n",
        "y_all_1 = db.label\n",
        "print(X_all_1.shape)\n",
        "print(y_all_1.shape)\n",
        "print('')\n",
        "\n",
        "print('Count Vectorizer')\n",
        "vectorizer_2 = CountVectorizer(stop_words='english')\n",
        "X_all_2 = vectorizer_2.fit_transform(db.text)\n",
        "y_all_2 = db.label\n",
        "print(X_all_2.shape)\n",
        "print(y_all_2.shape)\n",
        "print('')\n",
        "\n",
        "print('Tfidf Vectorizer')\n",
        "vectorizer_3 = TfidfVectorizer(stop_words='english')\n",
        "X_all_3 = vectorizer_3.fit_transform(db.text)\n",
        "y_all_3 = db.label \n",
        "print(X_all_3.shape)\n",
        "print(y_all_3.shape)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5572, 8444)\n",
            "(5572,)\n",
            "\n",
            "--- Homework\n",
            "Hashing Vectorizer\n",
            "(5572, 1048576)\n",
            "(5572,)\n",
            "\n",
            "Count Vectorizer\n",
            "(5572, 8444)\n",
            "(5572,)\n",
            "\n",
            "Tfidf Vectorizer\n",
            "(5572, 8444)\n",
            "(5572,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7oczFvA7nXb"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yw3Fz1NSu01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e8d911-6adb-45f1-b2e2-e00e58283994"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n",
        "          test_size=0.2, random_state=16)\n",
        "\n",
        "print(\"Train: %d - Test: %d\" %(X_train.shape[0],X_test.shape[0]))\n",
        "\n",
        "print('\\n---Homework')\n",
        "print('Split the hashing vector')\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_all_1, y_all_1, \n",
        "          test_size=0.2, random_state=16)\n",
        "\n",
        "print(\"Train: %d - Test: %d\\n\" %(X_train_1.shape[0], X_test_1.shape[0]))\n",
        "\n",
        "print('Split the count vector')\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_all_2, y_all_2, \n",
        "          test_size=0.2, random_state=16)\n",
        "\n",
        "print(\"Train: %d - Test: %d\\n\" %(X_train_2.shape[0], X_test_2.shape[0]))\n",
        "\n",
        "print('Split the tfidf vector')\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_all_3, y_all_3, \n",
        "          test_size=0.2, random_state=16)\n",
        "\n",
        "print(\"Train: %d - Test: %d\\n\" %(X_train_3.shape[0], X_test_3.shape[0]))\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4457 - Test: 1115\n",
            "\n",
            "---Homework\n",
            "Split the hashing vector\n",
            "Train: 4457 - Test: 1115\n",
            "\n",
            "Split the count vector\n",
            "Train: 4457 - Test: 1115\n",
            "\n",
            "Split the tfidf vector\n",
            "Train: 4457 - Test: 1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66oz8ep57xg"
      },
      "source": [
        "## Create and fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk4BBWhpUrQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f6f299-ca0c-4df1-f2fa-5d4d6e8a9fab"
      },
      "source": [
        "model_type = \"multinomial\" # \"bernoulli\" or \"multinomial\"\n",
        "\n",
        "if model_type == \"bernoulli\":\n",
        "  model = BernoulliNB().fit(X_train, y_train)\n",
        "  print('Bernoulli Model created')\n",
        "elif model_type == \"multinomial\":\n",
        "  model = MultinomialNB().fit(X_train, y_train)\n",
        "  print('Multinomial Model created')\n",
        "\n",
        "print('\\n---Homework')\n",
        "model_1_B = BernoulliNB().fit(X_train_1, y_train_1)\n",
        "# For hashing vector we can't use a Multinomial model because the vector\n",
        "# contains negative values.\n",
        "\n",
        "model_2_B = BernoulliNB().fit(X_train_2, y_train_2)\n",
        "model_2_M = MultinomialNB().fit(X_train_2, y_train_2)\n",
        "\n",
        "model_3_B = BernoulliNB().fit(X_train_3, y_train_3)\n",
        "model_3_M = MultinomialNB().fit(X_train_3, y_train_3)\n",
        "print('Models created!')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Model created\n",
            "\n",
            "---Homework\n",
            "Models created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBrexa46DmE"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2dR-PQtaiJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9364b0c3-9ded-4736-aaaf-0a56f7637614"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "#print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# with zero division on\n",
        "print(classification_report(y_test, y_pred, zero_division=1))\n",
        "\n",
        "\n",
        "print('\\n---Homework --------------------------------------')\n",
        "y_pred_1_B = model_1_B.predict(X_test_1)\n",
        "\n",
        "y_pred_2_B = model_2_B.predict(X_test_2)\n",
        "y_pred_2_M = model_2_M.predict(X_test_2)\n",
        "\n",
        "y_pred_3_B = model_3_B.predict(X_test_3)\n",
        "y_pred_3_M = model_3_M.predict(X_test_3)\n",
        "\n",
        "print('Classification for hashing vector - model Bernoulli:')\n",
        "print(classification_report(y_test_1, y_pred_1_B, zero_division=1))\n",
        "print('')\n",
        "\n",
        "print('Classification for count vector - model Bernoulli:')\n",
        "print(classification_report(y_test_2, y_pred_2_B, zero_division=1))\n",
        "print('')\n",
        "print('Classification for count vector - model Multinomial:')\n",
        "print(classification_report(y_test_2, y_pred_2_M, zero_division=1))\n",
        "print('')\n",
        "\n",
        "print('Classification for tfidf vector - model Bernoulli:')\n",
        "print(classification_report(y_test_3, y_pred_3_B, zero_division=1))\n",
        "print('')\n",
        "print('Classification for tfidf vector - model Multinomial:')\n",
        "print(classification_report(y_test_3, y_pred_3_M, zero_division=1))\n",
        "print('')\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       971\n",
            "        spam       0.94      0.93      0.93       144\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.96      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "---Homework --------------------------------------\n",
            "Classification for hashing vector - model Bernoulli:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.87      1.00      0.93       971\n",
            "        spam       1.00      0.00      0.00       144\n",
            "\n",
            "    accuracy                           0.87      1115\n",
            "   macro avg       0.94      0.50      0.47      1115\n",
            "weighted avg       0.89      0.87      0.81      1115\n",
            "\n",
            "\n",
            "Classification for count vector - model Bernoulli:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       971\n",
            "        spam       0.97      0.85      0.91       144\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.97      0.93      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "Classification for count vector - model Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       971\n",
            "        spam       0.94      0.93      0.93       144\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.96      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "Classification for tfidf vector - model Bernoulli:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       971\n",
            "        spam       0.97      0.85      0.91       144\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.97      0.93      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "Classification for tfidf vector - model Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       971\n",
            "        spam       1.00      0.78      0.88       144\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.89      0.93      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaiH-mlO6I-9"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD7Le-lVX9M8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38f594d-a449-48bb-cb50-65d032401155"
      },
      "source": [
        "smsnew1 = np.array(['Hello, what is your name?'])\n",
        "xnew1 = vectorizer.transform(smsnew1)\n",
        "ynew1 = model.predict(xnew1)\n",
        "print('%s %s' %(smsnew1,ynew1))\n",
        "\n",
        "smsnew2 = np.array(['Your account is blocked! Do login now'])\n",
        "xnew2 = vectorizer.transform(smsnew2)\n",
        "ynew2 = model.predict(xnew2)\n",
        "print('%s %s' %(smsnew2,ynew2))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello, what is your name?'] ['ham']\n",
            "['Your account is blocked! Do login now'] ['spam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYkT9M_N5Wb"
      },
      "source": [
        "## Home Exercises\n",
        "\n",
        "**[X] Question 1**\n",
        "\n",
        "`Design and implement an evaluation procedure to assess and compare the performance of the three vectorizers and the two models proposed above.`\n",
        "\n",
        "\n",
        "**Done.**"
      ]
    }
  ]
}